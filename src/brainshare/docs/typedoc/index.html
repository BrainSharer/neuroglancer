<!DOCTYPE html><html class="default" lang="en"><head><meta charset="utf-8"/><meta http-equiv="x-ua-compatible" content="IE=edge"/><title>neuroglancer</title><meta name="description" content="Documentation for neuroglancer"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="assets/style.css"/><link rel="stylesheet" href="assets/highlight.css"/><script defer src="assets/main.js"></script><script async src="assets/icons.js" id="tsd-icons-script"></script><script async src="assets/search.js" id="tsd-search-script"></script><script async src="assets/navigation.js" id="tsd-nav-script"></script></head><body><script>document.documentElement.dataset.theme = localStorage.getItem("tsd-theme") || "os";document.body.style.display="none";setTimeout(() => app?app.showPage():document.body.style.removeProperty("display"),500)</script><header class="tsd-page-toolbar"><div class="tsd-toolbar-contents container"><div class="table-cell" id="tsd-search" data-base="."><div class="field"><label for="tsd-search-field" class="tsd-widget tsd-toolbar-icon search no-caption"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-search"></use></svg></label><input type="text" id="tsd-search-field" aria-label="Search"/></div><div class="field"><div id="tsd-toolbar-links"></div></div><ul class="results"><li class="state loading">Preparing search index...</li><li class="state failure">The search index is not available</li></ul><a href="index.html" class="title">neuroglancer</a></div><div class="table-cell" id="tsd-widgets"><a href="#" class="tsd-widget tsd-toolbar-icon menu no-caption" data-toggle="menu" aria-label="Menu"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-menu"></use></svg></a></div></div></header><div class="container container-main"><div class="col-content"><div class="tsd-page-title"><h2>neuroglancer</h2></div><div class="tsd-panel tsd-typography"><a id="md:neuroglancer-web-based-volumetric-data-visualization" class="tsd-anchor"></a><h2 class="tsd-anchor-link">Neuroglancer: Web-based volumetric data visualization<a href="#md:neuroglancer-web-based-volumetric-data-visualization" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><p><a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License"></a>
<a href="https://pypi.org/project/neuroglancer"><img src="https://img.shields.io/pypi/v/neuroglancer" alt="PyPI"></a>
<img src="https://github.com/google/neuroglancer/workflows/Build/badge.svg" alt="Build">
<a href="https://zenodo.org/badge/latestdoi/59798355"><img src="https://zenodo.org/badge/59798355.svg" alt="DOI"></a></p>
<p>Neuroglancer is a WebGL-based viewer for volumetric data. It is capable of displaying arbitrary (non axis-aligned) cross-sectional views of volumetric data, as well as 3-D meshes and line-segment based models (skeletons).</p>
<p>This is not an official Google product.</p>
<a id="md:examples" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Examples<a href="#md:examples" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>A live demo is hosted at <a href="https://neuroglancer-demo.appspot.com">https://neuroglancer-demo.appspot.com</a>. (The prior link opens the viewer without any preloaded dataset.) Use the viewer links below to open the viewer preloaded with an example dataset.</p>
<p>The four-pane view consists of 3 orthogonal cross-sectional views as well as a 3-D view (with independent orientation) that displays 3-D models (if available) for the selected objects. All four views maintain the same center position. The orientation of the 3 cross-sectional views can also be adjusted, although they maintain a fixed orientation relative to each other. (Try holding the shift key and either dragging with the left mouse button or pressing an arrow key.)</p>
<ul>
<li>
<p><a href="https://www.janelia.org/project-team/flyem/hemibrain">FlyEM Hemibrain</a> (8x8x8 cubic nanometer resolution). <a href="https://hemibrain-dot-neuroglancer-demo.appspot.com/#!gs://neuroglancer-janelia-flyem-hemibrain/v1.0/neuroglancer_demo_states/base.json" target="_blank">Open viewer</a></p>
</li>
<li>
<p><a href="https://fafb-ffn1.storage.googleapis.com/landing.html">FAFB-FFN1 Full Adult Fly Brain Automated Segmentation</a> (4x4x40 cubic nanometer resolution). <a href="https://neuroglancer-demo.appspot.com/fafb.html#!gs://fafb-ffn1/main_ng.json" target="_blank">Open viewer</a></p>
</li>
<li>
<p>Kasthuri et al., 2014. Mouse somatosensory cortex (6x6x30 cubic nanometer resolution). <a href="https://neuroglancer-demo.appspot.com/#!{'layers':{'original-image':{'type':'image'_'source':'precomputed://gs://neuroglancer-public-data/kasthuri2011/image'_'visible':false}_'corrected-image':{'type':'image'_'source':'precomputed://gs://neuroglancer-public-data/kasthuri2011/image_color_corrected'}_'ground_truth':{'type':'segmentation'_'source':'precomputed://gs://neuroglancer-public-data/kasthuri2011/ground_truth'_'selectedAlpha':0.63_'notSelectedAlpha':0.14_'segments':['3208'_'4901'_'13'_'4965'_'4651'_'2282'_'3189'_'3758'_'15'_'4027'_'3228'_'444'_'3207'_'3224'_'3710']}}_'navigation':{'pose':{'position':{'voxelSize':[6_6_30]_'voxelCoordinates':[5523.99072265625_8538.9384765625_1198.0423583984375]}}_'zoomFactor':22.573112129999547}_'perspectiveOrientation':[-0.004047565162181854_-0.9566211104393005_-0.2268827110528946_-0.1827099621295929]_'perspectiveZoom':340.35867907175077}" target="_blank">Open viewer.</a></p>
<p>This dataset was copied from <a href="https://neurodata.io/data/kasthuri15/">https://neurodata.io/data/kasthuri15/</a> and is made available under the <a href="http://opendatacommons.org/licenses/by/1.0/">Open Data Common Attribution License</a>. Paper: <a href="http://dx.doi.org/10.1016/j.cell.2015.06.054" target="_blank">Kasthuri, Narayanan, et al. &quot;Saturated reconstruction of a volume of neocortex.&quot; Cell 162.3 (2015): 648-661.</a></p>
</li>
<li>
<p>Janelia FlyEM FIB-25. 7-column Drosophila medulla (8x8x8 cubic nanometer resolution). <a href="https://neuroglancer-demo.appspot.com/#!{'layers':{'image':{'type':'image'_'source':'precomputed://gs://neuroglancer-public-data/flyem_fib-25/image'}_'ground-truth':{'type':'segmentation'_'source':'precomputed://gs://neuroglancer-public-data/flyem_fib-25/ground_truth'_'segments':['21894'_'22060'_'158571'_'24436'_'2515']}}_'navigation':{'pose':{'position':{'voxelSize':[8_8_8]_'voxelCoordinates':[2914.500732421875_3088.243408203125_4045]}}_'zoomFactor':30.09748283999932}_'perspectiveOrientation':[0.3143535554409027_0.8142156600952148_0.4843369424343109_-0.06040262430906296]_'perspectiveZoom':443.63404517712684_'showSlices':false}" target="_blank">Open viewer.</a></p>
<p>This dataset was copied from <a href="https://www.janelia.org/project-team/flyem/data-and-software-release">https://www.janelia.org/project-team/flyem/data-and-software-release</a>, and is made available under the <a href="http://opendatacommons.org/licenses/by/1.0/">Open Data Common Attribution License</a>. Paper: <a href="http://dx.doi.org/10.1073/pnas.1509820112" target="_blank">Takemura, Shin-ya et al. &quot;Synaptic Circuits and Their Variations within Different Columns in the Visual System of Drosophila.&quot; Proceedings of the National Academy of Sciences of the United States of America 112.44 (2015): 13711-13716.</a></p>
</li>
<li>
<p>Example of viewing 2D microscopy (coronal section of rat brain at 325 nanometer resolution). <a href="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B1e-9%2C%22m%22%5D%2C%22y%22:%5B1e-9%2C%22m%22%5D%7D%2C%22position%22:%5B10387071%2C5347131%5D%2C%22crossSectionScale%22:263.74955563693914%2C%22projectionScale%22:65536%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22deepzoom://https://data-proxy.ebrains.eu/api/v1/buckets/localizoom/14122_mPPC_BDA_s186.tif/14122_mPPC_BDA_s186.dzi%22%2C%22transform%22:%7B%22outputDimensions%22:%7B%22x%22:%5B1e-9%2C%22m%22%5D%2C%22y%22:%5B1e-9%2C%22m%22%5D%2C%22c%5E%22:%5B1%2C%22%22%5D%7D%2C%22inputDimensions%22:%7B%22x%22:%5B3.25e-7%2C%22m%22%5D%2C%22y%22:%5B3.25e-7%2C%22m%22%5D%2C%22c%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22rendering%22%2C%22shader%22:%22void%20main%28%29%7BemitRGB%28vec3%28toNormalized%28getDataValue%280%29%29%2CtoNormalized%28getDataValue%281%29%29%2CtoNormalized%28getDataValue%282%29%29%29%29%3B%7D%22%2C%22channelDimensions%22:%7B%22c%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%2214122_mPPC_BDA_s186.dzi%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%2214122_mPPC_BDA_s186.dzi%22%7D%2C%22layout%22:%22xy%22%7D"
target="_blank">Open viewer.</a> (Use <kbd>Ctrl</kbd>+<kbd>MouseWheel</kbd> to zoom out)</p>
<p>This image is part of: Olsen et al., 2020. Anterogradely labeled axonal projections from the posterior parietal cortex in rat [Data set]. EBRAINS. <a href="https://doi.org/10.25493/FKM4-ZCC">https://doi.org/10.25493/FKM4-ZCC</a></p>
</li>
</ul>
<a id="md:supported-data-sources" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Supported data sources<a href="#md:supported-data-sources" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>Neuroglancer itself is purely a client-side program, but it depends on data being accessible via HTTP in a suitable format. It is designed to easily support many different data sources, and there is existing support for the following data APIs/formats:</p>
<ul>
<li><a href="src/datasource/precomputed">Neuroglancer precomputed format</a></li>
<li><a href="src/datasource/n5">N5</a></li>
<li><a href="src/datasource/zarr">Zarr v2/v3</a></li>
<li><a href="media/README.md">Python in-memory volumes</a> (with automatic mesh generation)</li>
<li>BOSS <a href="https://bossdb.org/">https://bossdb.org/</a></li>
<li>DVID <a href="https://github.com/janelia-flyem/dvid">https://github.com/janelia-flyem/dvid</a></li>
<li>Render <a href="https://github.com/saalfeldlab/render">https://github.com/saalfeldlab/render</a></li>
<li>Single NIfTI files <a href="https://www.nitrc.org/projects/nifti">https://www.nitrc.org/projects/nifti</a></li>
<li><a href="src/datasource/deepzoom">Deep Zoom images</a></li>
</ul>
<a id="md:supported-browsers" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Supported browsers<a href="#md:supported-browsers" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><ul>
<li>Chrome &gt;= 51</li>
<li>Firefox &gt;= 46</li>
<li>Safari &gt;= 15.0</li>
</ul>
<a id="md:keyboard-and-mouse-bindings" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Keyboard and mouse bindings<a href="#md:keyboard-and-mouse-bindings" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>For the complete set of bindings, see
<a href="modules/ui_default_input_event_bindings.html">src/ui/default_input_event_bindings.ts</a>,
or within Neuroglancer, press <code>h</code> or click on the button labeled <code>?</code> in the upper right corner.</p>
<ul>
<li>
<p>Click on a layer name to toggle its visibility.</p>
</li>
<li>
<p>Double-click on a layer name to edit its properties.</p>
</li>
<li>
<p>Hover over a segmentation layer name to see the current list of objects shown and to access the opacity sliders.</p>
</li>
<li>
<p>Hover over an image layer name to access the opacity slider and the text editor for modifying the <a href="media/image_layer_rendering.md">rendering code</a>.</p>
</li>
</ul>
<a id="md:troubleshooting" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Troubleshooting<a href="#md:troubleshooting" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><ul>
<li>
<p>Neuroglancer doesn't appear to load properly.</p>
<p>Neuroglancer requires WebGL (2.0) and the <code>EXT_color_buffer_float</code> extension.</p>
<p>To troubleshoot, check the developer console, which is accessed by the keyboard shortcut <code>control-shift-i</code> in Firefox and Chrome. If there is a message regarding failure to initialize WebGL, you can take the following steps:</p>
<ul>
<li>
<p>Chrome</p>
<p>Check <code>chrome://gpu</code> to see if your GPU is blacklisted. There may be a flag you can enable to make it work.</p>
</li>
<li>
<p>Firefox</p>
<p>Check <code>about:support</code>. There may be webgl-related properties in <code>about:config</code> that you can change to make it work. Possible settings:</p>
<ul>
<li><code>webgl.disable-fail-if-major-performance-caveat = true</code></li>
<li><code>webgl.force-enabled = true</code></li>
<li><code>webgl.msaa-force = true</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Failure to access a data source.</p>
<p>As a security measure, browsers will in many prevent a webpage from accessing the true error code associated with a failed HTTP request. It is therefore often necessary to check the developer tools to see the true cause of any HTTP request error.</p>
<p>There are several likely causes:</p>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Cross-origin_resource_sharing">Cross-origin resource sharing (CORS)</a></p>
<p>Neuroglancer relies on cross-origin requests to retrieve data from third-party servers. As a security measure, if an appropriate <code>Access-Control-Allow-Origin</code> response header is not sent by the server, browsers prevent webpages from accessing any information about the response from a cross-origin request. In order to make the data accessible to Neuroglancer, you may need to change the cross-origin request sharing (CORS) configuration of the HTTP server.</p>
</li>
<li>
<p>Accessing an <code>http://</code> resource from a Neuroglancer client hosted at an <code>https://</code> URL</p>
<p>As a security measure, recent versions of Chrome and Firefox prohibit webpages hosted at <code>https://</code> URLs from issuing requests to <code>http://</code> URLs. As a workaround, you can use a Neuroglancer client hosted at a <code>http://</code> URL, e.g. the demo client running at <a href="http://neuroglancer-demo.appspot.com">http://neuroglancer-demo.appspot.com</a>, or one running on localhost. Alternatively, you can start Chrome with the <code>--disable-web-security</code> flag, but that should be done only with extreme caution. (Make sure to use a separate profile, and do not access any untrusted webpages when running with that flag enabled.)</p>
</li>
</ul>
</li>
</ul>
<a id="md:multi-threaded-architecture" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Multi-threaded architecture<a href="#md:multi-threaded-architecture" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>In order to maintain a responsive UI and data display even during rapid navigation, work is split between the main UI thread (referred to as the &quot;frontend&quot;) and a separate WebWorker thread (referred to as the &quot;backend&quot;). This introduces some complexity due to the fact that current browsers:</p>
<ul>
<li>do not support any form of <em>shared</em> memory or standard synchronization mechanism (although they do support relatively efficient <em>transfers</em> of typed arrays between threads);</li>
<li>require that all manipulation of the DOM and the WebGL context happens on the main UI thread.</li>
</ul>
<p>The &quot;frontend&quot; UI thread handles user actions and rendering, while the &quot;backend&quot; WebWorker thread handle all queuing, downloading, and preprocessing of data needed for rendering.</p>
<a id="md:documentation-index" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Documentation Index<a href="#md:documentation-index" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><ul>
<li><a href="media/image_layer_rendering.md">Image Layer Rendering</a></li>
<li><a href="media/README-1.md">Cross-sectional view implementation architecture</a></li>
<li><a href="media/README-2.md">Compressed segmentation format</a></li>
<li><a href="src/chunk_manager/">Data chunk management</a></li>
<li><a href="src/gpu_hash/">On-GPU hashing</a></li>
</ul>
<a id="md:building" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Building<a href="#md:building" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>node.js is required to build the viewer.</p>
<ol>
<li>First install NVM (node version manager) per the instructions here:</li>
</ol>
<p><a href="https://github.com/creationix/nvm">https://github.com/creationix/nvm</a></p>
<ol start="2">
<li>
<p>Install a recent version of Node.js if you haven't already done so:</p>
<p><code>nvm install stable</code></p>
</li>
<li>
<p>Install the dependencies required by this project:</p>
<p>(From within this directory)</p>
<p><code>npm i</code></p>
<p>Also re-run this any time the dependencies listed in <a href="media/package.json">package.json</a> may have
changed, such as after checking out a different revision or pulling changes.</p>
</li>
<li>
<p>To run a local server for development purposes:</p>
<p><code>npm run dev-server</code></p>
<p>This will start a server on <a href="http://localhost:8080">http://localhost:8080</a>.</p>
</li>
<li>
<p>To run the unit test suite on Chrome:</p>
<p><code>npm test</code></p>
<p>To run only tests in files matching a given glob pattern:</p>
<p><code>npm test -- --pattern='&lt;pattern&gt;'</code></p>
<p>For example,</p>
<p><code>npm test -- --pattern='src/util/uint64*'</code></p>
</li>
<li>
<p>See <a href="media/package.json">package.json</a> for other commands available.</p>
</li>
</ol>
<a id="md:discussion-group" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Discussion Group<a href="#md:discussion-group" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>There is a Google Group/mailing list for discussion related to Neuroglancer:
<a href="https://groups.google.com/forum/#!forum/neuroglancer">https://groups.google.com/forum/#!forum/neuroglancer</a>.</p>
<a id="md:related-projects" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Related Projects<a href="#md:related-projects" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><ul>
<li><a href="https://github.com/google/tensorstore">TensorStore</a> - C++ and Python library for efficiently
reading and writing multi-dimensional arrays in formats supported by Neuroglancer.</li>
<li><a href="https://github.com/4Quant/neuroglancer-docker">4Quant/neuroglancer-docker</a> - Example setup for
Docker deployment of the <a href="media/README.md">Neuroglancer Python integration</a>.</li>
<li><a href="https://github.com/FZJ-INM1-BDA/neuroglancer-scripts">FZJ-INM1-BDA/neuroglancer-scripts</a> -
Scripts for converting the <a href="https://bigbrain.loris.ca">BigBrain</a> dataset to the
Neuroglancer <a href="src/datasource/precomputed">precomputed data format</a>, which may serve
as a useful example for converting other datasets.</li>
<li><a href="https://github.com/seung-lab/BigArrays.jl">BigArrays.jl</a> - Julia interface of neuroglancer precomputed data format.</li>
<li><a href="https://github.com/seung-lab/cloud-volume">cloudvolume</a> - Python interface of neuroglancer precomputed data format.</li>
<li><a href="https://github.com/janelia-cosem/multiresolution-mesh-creator">multiresolution-mesh-creator</a> - Python tool for creating <a href="https://github.com/google/neuroglancer/blob/master/src/datasource/precomputed/meshes.md#multi-resolution-mesh-format">multi-resolution meshes</a> from single resolution - or multiscale - meshes.</li>
<li><a href="https://github.com/seung-lab/igneous">Igneous</a> - Python pipeline for scalable meshing, skeletonizing, downsampling, and managment of large 3d images focusing on Neuroglancer Precomputed format.</li>
</ul>
<a id="md:contributing" class="tsd-anchor"></a><h1 class="tsd-anchor-link">Contributing<a href="#md:contributing" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>Want to contribute? Great! First, read <a href="media/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<a id="md:license" class="tsd-anchor"></a><h1 class="tsd-anchor-link">License<a href="#md:license" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p>Copyright 2016 Google Inc.</p>
<p>Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this software except in compliance with the License.
You may obtain a copy of the License at <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a>.</p>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
</div></div><div class="col-sidebar"><div class="page-menu"><div class="tsd-navigation settings"><details class="tsd-accordion"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>Settings</h3></summary><div class="tsd-accordion-details"><div class="tsd-filter-visibility"><span class="settings-label">Member Visibility</span><ul id="tsd-filter-options"><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-protected" name="protected"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Protected</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-private" name="private"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Private</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-inherited" name="inherited" checked/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Inherited</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-external" name="external"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>External</span></label></li></ul></div><div class="tsd-theme-toggle"><label class="settings-label" for="tsd-theme">Theme</label><select id="tsd-theme"><option value="os">OS</option><option value="light">Light</option><option value="dark">Dark</option></select></div></div></details></div><details open class="tsd-accordion tsd-page-navigation"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>On This Page</h3></summary><div class="tsd-accordion-details"><ul><li><a href="#md:neuroglancer-web-based-volumetric-data-visualization"><span>Neuroglancer: <wbr/>Web-<wbr/>based volumetric data visualization</span></a></li></ul><a href="#md:examples"><span>Examples</span></a><a href="#md:supported-data-sources"><span>Supported data sources</span></a><a href="#md:supported-browsers"><span>Supported browsers</span></a><a href="#md:keyboard-and-mouse-bindings"><span>Keyboard and mouse bindings</span></a><a href="#md:troubleshooting"><span>Troubleshooting</span></a><a href="#md:multi-threaded-architecture"><span>Multi-<wbr/>threaded architecture</span></a><a href="#md:documentation-index"><span>Documentation <wbr/>Index</span></a><a href="#md:building"><span>Building</span></a><a href="#md:discussion-group"><span>Discussion <wbr/>Group</span></a><a href="#md:related-projects"><span>Related <wbr/>Projects</span></a><a href="#md:contributing"><span>Contributing</span></a><a href="#md:license"><span>License</span></a></div></details></div><div class="site-menu"><nav class="tsd-navigation"><a href="index.html" class="current"><svg class="tsd-kind-icon" viewBox="0 0 24 24"><use href="assets/icons.svg#icon-1"></use></svg><span>neuroglancer</span></a><ul class="tsd-small-nested-navigation" id="tsd-nav-container" data-base="."><li>Loading...</li></ul></nav></div></div></div><footer><p class="tsd-generator">Generated using <a href="https://typedoc.org/" target="_blank">TypeDoc</a></p></footer><div class="overlay"></div></body></html>
